%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,xcolor=svgnames, handout]{beamer}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{amstext}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
%\usepackage{multimedia}
\usepackage{movie15}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}

\makeatother

\usepackage{babel}
\begin{document}
\title[TDAB01]{Sannolikhetslära och statistik Föreläsning 1}
\author[Mattias Villani]{Mattias Villani}
\institute[\textbf{STIMA, LiU}]{\textbf{Avdelningen för Statistik och Maskininlärning}\\
\textbf{Institutionen för datavetenskap}\\
\textbf{Linköpings universitet }}
\date{\,}
\makebeamertitle
\begin{frame}{Overview}

\begin{itemize}
\item \textbf{\textcolor{blue}{Multivariate normal}}
\item \textbf{\textcolor{blue}{Gaussian process regression}}
\end{itemize}

\pause{}

\begin{figure}
\includemovie[poster, text={\small(spot.mp4)} ]{5.25cm}{3.5cm}{spot.mp4}
\includemovie[poster, text={\small(robocup.mp4)} ]{5.25cm}{3.5cm}{robocup.mp4}
\end{figure}

\end{frame}

\begin{frame}{Exempel - Robotik}

\begin{itemize}
\item \textbf{\textcolor{blue}{Lokalisering}}

\begin{itemize}
\item \textcolor{black}{Robot: }\textbf{\textcolor{blue}{'Var är jag?'}}
\item Sannolikhetsfördelning över positioner som uppdateras varefter med
brusiga sensordata.
\end{itemize}
\item \textbf{\textcolor{blue}{Räddningsrobot }}

\begin{itemize}
\item Ta sig till olycksplats med objekt i vägen, t ex väggar eller människor.
\item K olika vägar, alla med olika förväntade färdtider. Osäkerhet.
\end{itemize}
\end{itemize}
\includemovie[poster, text={\small(spot.mp4)} ]{5.25cm}{3.5cm}{spot.mp4}
\includemovie[poster, text={\small(robocup.mp4)} ]{5.25cm}{3.5cm}{robocup.mp4}

\begin{figure}[b]
\includemovie[poster, text={\small(spot.mp4)} ]{5.25cm}{3.5cm}{spot.mp4}
\includemovie[poster, text={\small(robocup.mp4)} ]{5.25cm}{3.5cm}{robocup.mp4}
\end{figure}

\end{frame}

\begin{frame}{Multivariate normal - some properties}

\begin{itemize}
\item Let $\mathbf{x}\sim N_{p}(\mu,\Sigma)$.
\item Let $\mathbf{x}=\left(\begin{array}{c}
\mathbf{x}_{1}\\
\mathbf{x}_{2}
\end{array}\right)$ where $\mathbf{x}_{1}$ is $p_{1}\times1$ and $\mathbf{x}_{2}$
is $p_{2}\times1$ ($p_{1}+p_{2}=p$). 
\item Let $\mu=\left(\begin{array}{c}
\mu_{1}\\
\mu_{2}
\end{array}\right)$ and 
\[
\Sigma=\left(\begin{array}{cc}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)
\]
\item \textbf{\textcolor{blue}{Marginal distributions}} are normal
\[
\mathbf{x}_{1}\sim N_{p_{1}}(\mu_{1},\Sigma_{1})
\]
\item \textbf{\textcolor{blue}{Conditional distributions }}are normal
\[
\mathbf{x}_{1}|\mathbf{x}_{2}\sim N_{p_{1}}\left[\mu_{1}+\Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2}),\;\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right]
\]
\end{itemize}
\end{frame}

\begin{frame}{Non-parametric regression}

\begin{itemize}
\item \textbf{\textcolor{blue}{Linear regression}}
\[
y=\beta\cdot x+\varepsilon
\]
where $\varepsilon\sim N(0,\sigma^{2})$ and iid over observations.
\item \textbf{\textcolor{blue}{Nonlinear}}\textcolor{blue}{{} }\textbf{\textcolor{blue}{regression}}
\[
y=f(x)+\varepsilon
\]
where $f(\cdot)$ is some nonlinear function (ex $f(x)=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}$).
\end{itemize}

\pause{}
\begin{itemize}
\item \textbf{\textcolor{blue}{Non-parametric regression}}: avoiding a parametric
form for $f(\cdot)$.
\item How do we put a \textbf{\textcolor{blue}{prior over a set of functions}}?
\item Restrict attention to a grid of (ordered) $x$-values: $x_{1},x_{2},..,x_{k}$.
\item We can now put a joint prior on the $k$ function values: $f(x_{1}),f(x_{2}),...,f(x_{k})$.
\end{itemize}
\end{frame}

\begin{frame}{Nonparametric = one parameter for every x!}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncSmoothManyPoints.svg}
\par\end{center}

\end{frame}

\begin{frame}{Gaussian process regression}

\begin{itemize}
\item We clearly need to impose \textbf{\textcolor{blue}{smoothness}}. 
\item Multivariate normal (Gaussian) prior:
\[
\left(\begin{array}{c}
f(x_{1})\\
\vdots\\
f(x_{k})
\end{array}\right)\sim N\left(\mathbf{m},\mathbf{K}\right)
\]
\item But how do we specify the $k\times k$ \textbf{\textcolor{blue}{covariance
matrix}}\textcolor{blue}{{} }$\mathbf{K}$?
\[
Cov\left(f(x_{p}),f(x_{q})\right)
\]
\end{itemize}

\pause{}
\begin{itemize}
\item \textbf{\textcolor{blue}{Squared exponential covariance function}}
\[
Cov\left(f(x_{p}),f(x_{q})\right)=K(x_{p},x_{q})=\sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\frac{x_{p}-x_{q}}{\ell}\right)^{2}\right)
\]
\item The covariance between $f(x_{p})$ and \textrm{$f(x_{q})$} is a function
of $x_{p}$ and $x_{q}$.
\item Nearby $x$'s have highly correlated function ordinates $f(x)$\textrm{.}
\end{itemize}
\end{frame}

\begin{frame}{Smooth function - points nearby}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncSmoothTwoPointsNear.svg}
\par\end{center}

\end{frame}

\begin{frame}{Smooth function - points nearby}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncSmoothTwoPointsNearWithDensity.svg}
\par\end{center}

\end{frame}

\begin{frame}{Smooth function - points far apart}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncSmoothTwoPointsApart.svg}
\par\end{center}

\end{frame}

\begin{frame}{Smooth function - points far apart}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncSmoothTwoPointsApartWithDensity.svg}
\par\end{center}

\end{frame}

\begin{frame}{Jagged function - points nearby}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncJaggedTwoPointsNear.svg}
\par\end{center}

\end{frame}

\begin{frame}{Jagged function - points nearby}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncJaggedTwoPointsNearWithDensity.svg}
\par\end{center}

\end{frame}

\begin{frame}{Jagged function - points far apart}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncJaggedTwoPointsApart.svg}
\par\end{center}

\end{frame}

\begin{frame}{Jagged function - points far apart}

\noindent \begin{center}
\includegraphics[clip,scale=0.5,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPFuncJaggedTwoPointsApartWithDensity.svg}
\par\end{center}

\end{frame}

\begin{frame}{Gaussian process regression, cont.}

\begin{definition}
A \textbf{\textcolor{blue}{Gaussian process}} (\textbf{\textcolor{blue}{GP}})
is a collection of random variables, any finite number of which have
a multivariate Gaussian distribution.
\end{definition}

\begin{itemize}
\item A Gaussian process is a \textbf{\textcolor{blue}{probability distribution
over functions}}.\medskip{}
\item A GP is completely specified by a \textbf{mean} and a \textbf{covariance
function}
\[
m(x)=\mathrm{E}\left[f(x)\right]
\]
\[
K(x,x')=E\left[\left(f(x)-m(x)\right)\left(f(x')-m(x')\right)\right]
\]
for any two inputs $x$ and $x'$ (note: this is \emph{not} the transpose
here).\medskip{}
\item A \textbf{\textcolor{blue}{Gaussian process}} (prior) is denoted by
\[
f(x)\sim GP\left(m(x),K(x,x')\right)
\]
\end{itemize}
\end{frame}

\begin{frame}{Gaussian process regression, cont.}

\begin{itemize}
\item Example:
\begin{align*}
m(x) & =\sin(x)\\
K(x,x') & =\sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\frac{x_{p}-x_{q}}{\ell}\right)^{2}\right)
\end{align*}
where $\ell>0$ is the length scale. \medskip{}
\item Larger $\ell$ gives more smoothness in $f(x)$.\medskip{}
\item \textbf{\textcolor{blue}{Simulate}} a draw from $f(x)\sim GP\left(m(x),K(x,x')\right)$
over any grid $x_{*}=(x_{1},...,x_{n})$ by using that
\[
f(x_{*})\sim N\left(m(x_{*}),K(x_{*},x_{*})\right)
\]
\end{itemize}
\end{frame}

\begin{frame}{Simulating a GP - sine mean and SE kernel}

\begin{center}
\includegraphics[scale=0.21,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPPriorDraws_l2_sigmaf1.svg}\includegraphics[scale=0.21,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPPriorDraws_l1_sigmaf1.svg}
\par\end{center}

\begin{center}
\includegraphics[scale=0.21,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPPriorDraws_l05_sigmaf1.svg}\includegraphics[scale=0.21,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPPriorDraws_l02_sigmaf1.svg}
\par\end{center}

\begin{center}
\includegraphics[scale=0.21,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPPriorDraws_l1_sigmaf02.svg}\includegraphics[scale=0.21,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/GPPriorDraws_l2_sigmaf02.svg}
\par\end{center}

\end{frame}

\begin{frame}{Gaussian process regression, cont.}

\begin{itemize}
\item \textbf{\textcolor{blue}{Model}}
\[
y_{i}=f(\mathbf{\textnormal{x}}_{i})+\varepsilon_{i},\quad\varepsilon\overset{iid}{\sim}N(0,\sigma^{2})
\]
\item \textbf{\textcolor{blue}{Prior}}
\[
f(x)\sim GP\left(0,K(x,x')\right)
\]
\item You have observed the data: $\mathbf{x}=(x_{1},...,x_{n})$' and $\mathbf{y}=(y_{1},...,y_{n})'$. 
\item Goal: the posterior of $f(\cdot)$ over a grid of $x$-values:
\[
\mathbf{f}_{*}=\mathbf{f}(\mathbf{x}_{*})=\left(f(x_{1*}),f(x_{2*}),...,f(x_{m*})\right)
\]
\end{itemize}

\pause{}
\begin{itemize}
\item The \textbf{\textcolor{blue}{posterior}} 
\[
\mathbf{f}_{*}|\mathbf{x},\mathbf{y},\mathbf{x}_{*}\sim N\left(\bar{\mathbf{f}}_{*},\mathrm{cov}(\mathbf{f}_{*})\right)
\]
\begin{align*}
\bar{\mathbf{f}}_{*} & =K(\mathbf{x}_{*},\mathbf{x})\left[K(\mathbf{x},\mathbf{x})+\sigma^{2}I\right]^{-1}\mathbf{y}\\
\mathrm{cov}(\mathbf{f}_{*}) & =K(\mathbf{x}_{*},\mathbf{x_{*}})-K(\mathbf{x}_{*},\mathbf{x})\left[K(\mathbf{x},\mathbf{x})+\sigma^{2}I\right]^{-1}K(\mathbf{x},\mathbf{x_{*}})
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Learning a noise-free Gaussian process}

\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll1_0.eps}\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll02_0.eps}
\end{frame}

\begin{frame}{Learning a noise-free Gaussian process}

\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll1_1.eps}\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll02_1.eps}
\end{frame}

\begin{frame}{Learning a noise-free Gaussian process}

\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll1_2.eps}\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll02_2.eps}
\end{frame}

\begin{frame}{Learning a noise-free Gaussian process}

\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll1_3.eps}\includegraphics[scale=0.32,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/ConditionalSimEll02_3.eps}
\end{frame}

\begin{frame}{Example - Canadian wages}

\begin{center}
\includegraphics[scale=0.45,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/CanadianWages.eps}
\par\end{center}

\end{frame}

\begin{frame}{Posterior of f - $\ell=0.2,0.5,1,2$}

\begin{center}
\includegraphics[scale=0.25,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/CanadianIntervalsF.eps}\includegraphics[scale=0.25,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/CanadianIntervalsF_ell05.eps}
\par\end{center}

\begin{center}
\includegraphics[scale=0.25,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/CanadianIntervalsF_ell1.eps}\includegraphics[scale=0.25,bb = 0 0 200 100, draft, type=eps]{../../BayesLearning/Slides/CanadianIntervalsF_ell2.eps}
\par\end{center}

\end{frame}


\end{document}
