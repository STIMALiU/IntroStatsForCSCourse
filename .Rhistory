plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost$fPostMean
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(10,0.2), sigmaNoise = 0.01, xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(10,0.2), sigmaNoise = 0.1, xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(10,0.2), sigmaNoise = 0.5, xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(10,0.2), sigmaNoise = 100, xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(10,0.2), sigmaNoise = 30, xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
japanTemp <- read.table('~/Dropbox/Teaching/IntroToML/GPandMixtures/Lab/JapanTemp.dat', header = TRUE)
y <- japanTemp$temp
x <- japanTemp$time
xStar <- x
lmModel <- lm(y ~ x)
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(1,0.2), sigmaNoise = sd(lmModel$residuals), xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(10,0.2), sigmaNoise = sd(lmModel$residuals), xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(100,0.2), sigmaNoise = sd(lmModel$residuals), xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=1){
n1 <- length(x1)
n2 <- length(x2)
K <- matrix(NA,n1,n2)
for (i in 1:n2){
K[,i] <- sigmaF*exp(-0.5*( (x1-x2[i])/l)^2 )
}
return(K)
}
# Defining the PosteriorGP function that computes the posterior mean and variance
PosteriorGP <- function(x, y, K=SquaredExpKernel, hyperParam, sigmaNoise, xStar=x){
k_x_x <- K(x,x,hyperParam[1],hyperParam[2])
k_x_xStar <- K(x,xStar,hyperParam[1],hyperParam[2])
k_xStar_x <- K(xStar,x,hyperParam[1],hyperParam[2])
k_xStar_xStar <- K(xStar,xStar,hyperParam[1],hyperParam[2])
nx <- ncol(k_x_x)
# Calculate the mean and covariance functions
fPostMean <- k_xStar_x%*%solve(k_x_x + sigmaNoise^2*diag(1,nx))%*%y
fPostCov <- k_xStar_xStar - k_xStar_x%*%solve(k_x_x + sigmaNoise^2*diag(1,nx))%*%k_x_xStar
return(list(fPostMean = fPostMean, fPostCov = fPostCov))
}
# 1 b)
# Data
x <- c(-1.0,-0.6,-0.2,0.4,0.8)
y <- c(0.768,-0.044,-0.940,0.719,-0.664)
# Only first observation
xStar <- seq(-1,1,by =0.01)
GPPost <- PosteriorGP(x[4], y[4], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 1 c)
# Only two first observations
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x[c(2,4)], y[c(2,4)], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 1 d)
# Data
x <- c(-1.0,-0.6,-0.2,0.4,0.8)
y <- c(0.768,-0.044,-0.940,0.719,-0.664)
# Only first observation
xStar <- seq(-1,1,by =0.01)
GPPost <- PosteriorGP(x[4], y[4], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# Data
x <- c(-1.0,-0.6,-0.2,0.4,0.8)
y <- c(0.768,-0.044,-0.940,0.719,-0.664)
# Only first observation
xStar <- seq(-1,1,by =0.01)
GPPost <- PosteriorGP(x[4], y[4], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 1 c)
# Only two first observations
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x[c(2,4)], y[c(2,4)], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 1 d)
# All 5 observations
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 1 e)
# Repeat with lengthScale = 1
# Only first observation
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x[4], y[4], K=SquaredExpKernel, hyperParam=c(1,1), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# Only two first observations
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x[c(2,4)], y[c(2,4)], K=SquaredExpKernel, hyperParam=c(1,1), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# All 5 observations
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(1,1), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 2) Japan temp data
# read data
japanTemp <- read.table('~/Dropbox/Teaching/IntroToML/GPandMixtures/Lab/JapanTemp.dat', header = TRUE)
y <- japanTemp$temp
x <- japanTemp$time
xStar <- x
lmModel <- lm(y ~ x)
GPPost <- PosteriorGP(x, y, K=SquaredExpKernel, hyperParam=c(100,0.2), sigmaNoise = sd(lmModel$residuals), xStar)
plot(x,y, cex=0.4)
lines(xStar,GPPost$fPostMean, type="l", col="blue", lwd = 3)
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", lwd = 2, col="red")
################################################
# Problem 2 - EM for mixture models
################################################
# 2 a)
mixtureEM <- function(data, K, initMu, initSigma, initPi, tol){
# Preliminaries
count <- 0
nTotal <- length(data)
N <- rep(0,K)
gamma = matrix(0,nTotal,K)
Mu = initMu
Sigma = initSigma
Pi = initPi
LogLOld <- 10^10
LogLDiff <- 10^10
while (LogLDiff > tol){
count <- count + 1
# E-step
for (k in 1:K){
gamma[,k] = Pi[k]*dnorm(data, mean=Mu[k], sd = sqrt(Sigma[k]))
}
sumGamma <- rowSums(gamma)
for (k in 1:K){
gamma[,k] = gamma[,k]/sumGamma
}
# M-step
for (k in 1:K){
N[k] <- sum(gamma[,k])
Mu[k] = (1/N[k])*sum(gamma[,k]*data)
Sigma[k] = sqrt((1/N[k])*sum(gamma[,k]*(data-Mu[k])^2))
Pi[k] = N[k]/nTotal
}
''
''
â‰ˆ]
diag(5)
posteriorGP <- function(x,y,xStar,hyperParam,sigmaNoise){  N <- length(x)  mean = SquaredExpKernel(xStar,x,sigmaF = hyperParam[1],l =                            hyperParam[2])%*%solve(SquaredExpKernel(x,x,sigmaF = hyperParam[1],l =                                                                      hyperParam[2])+(sigmaNoise^2)*diag(N)) %*% y  cov = SquaredExpKernel(xStar,xStar,sigmaF = hyperParam[1],l = hyperParam[2]) -    SquaredExpKernel(xStar,x,sigmaF = hyperParam[1],l =                       hyperParam[2])%*%solve(SquaredExpKernel(x,x,sigmaF = hyperParam[1],l =                                                                 hyperParam[2])+sigmaNoise*diag(N)) %*% SquaredExpKernel(x,xStar,sigmaF = hyperParam[1],l =                                                                                                                           hyperParam[2])  return(list(mean = mean,cov = cov))}# KernelSquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){  n1 <- length(x1)  n2 <- length(x2)  K <- matrix(NA,n1,n2)  for (i in 1:n2){    K[,i] <- (sigmaF^2)*exp(-0.5*( (x1-x2[i])/l)^2 )  }  return(K)}
posteriorGP <- function(x,y,xStar,hyperParam,sigmaNoise){  N <- length(x)  mean = SquaredExpKernel(xStar,x,sigmaF = hyperParam[1],l =                            hyperParam[2])%*%solve(SquaredExpKernel(x,x,sigmaF = hyperParam[1],l =                                                                      hyperParam[2])+(sigmaNoise^2)*diag(N)) %*% y  cov = SquaredExpKernel(xStar,xStar,sigmaF = hyperParam[1],l = hyperParam[2]) -    SquaredExpKernel(xStar,x,sigmaF = hyperParam[1],l =                       hyperParam[2])%*%solve(SquaredExpKernel(x,x,sigmaF = hyperParam[1],l =                                                                 hyperParam[2])+sigmaNoise*diag(N)) %*% SquaredExpKernel(x,xStar,sigmaF = hyperParam[1],l =                                                                                                                           hyperParam[2])  return(list(mean = mean,cov = cov))}
# Setting up the squared exponential kernel
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=1){
n1 <- length(x1)
n2 <- length(x2)
K <- matrix(NA,n1,n2)
for (i in 1:n2){
K[,i] <- sigmaF*exp(-0.5*( (x1-x2[i])/l)^2 )
}
return(K)
}
# Defining the PosteriorGP function that computes the posterior mean and variance
PosteriorGP <- function(x, y, K=SquaredExpKernel, hyperParam, sigmaNoise, xStar=x){
k_x_x <- K(x,x,hyperParam[1],hyperParam[2])
k_x_xStar <- K(x,xStar,hyperParam[1],hyperParam[2])
k_xStar_x <- K(xStar,x,hyperParam[1],hyperParam[2])
k_xStar_xStar <- K(xStar,xStar,hyperParam[1],hyperParam[2])
nx <- ncol(k_x_x)
# Calculate the mean and covariance functions
fPostMean <- k_xStar_x%*%solve(k_x_x + sigmaNoise^2*diag(1,nx))%*%y
fPostCov <- k_xStar_xStar - k_xStar_x%*%solve(k_x_x + sigmaNoise^2*diag(1,nx))%*%k_x_xStar
return(list(fPostMean = fPostMean, fPostCov = fPostCov))
}
# 1 b)
# Data
x <- c(-1.0,-0.6,-0.2,0.4,0.8)
y <- c(0.768,-0.044,-0.940,0.719,-0.664)
# Only first observation
xStar <- seq(-1,1,by =0.01)
GPPost <- PosteriorGP(x[4], y[4], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
# 1 c)
# Only two first observations
xStar <- seq(-1,1,by =.1)
GPPost <- PosteriorGP(x[c(2,4)], y[c(2,4)], K=SquaredExpKernel, hyperParam=c(1,0.3), sigmaNoise = 0.1, xStar)
plot(x,y, ylim = c(-3,3))
lines(xStar,GPPost$fPostMean,type="l")
lines(xStar,GPPost$fPostMean - 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
lines(xStar,GPPost$fPostMean + 1.96*sqrt(diag(GPPost$fPostCov)),type="l", col="red")
SquaredExpKernel <- function(x1, x2, hyperParam){    sigmaF <- hyperParam[1]    el <- hyperParam[2]    n1 <- length(x1)    n2 <- length(x2)    K <- matrix(NA,n1,n2)    for (i in 1:n2) { K[,i] <- sigmaF^2 * exp(-0.5*( (x1 - x2[i]) / el)^2 ) }    return(K)}
SquaredExpKernel <- function(x1, x2, hyperParam){
sigmaF <- hyperParam[1]
el <- hyperParam[2]
n1 <- length(x1)
n2 <- length(x2)
K <- matrix(NA,n1,n2)
for (i in 1:n2) { K[,i] <- sigmaF^2 * exp(-0.5*( (x1 - x2[i]) / el)^2 ) }
return(K)
}
# GP parameters of posterior Normal of f(x)
posteriorGP <- function(x, y, xStar, hyperParam, sigmaNoise) {
K.prior <- SquaredExpKernel(x, x, hyperParam)
K.star.x <- SquaredExpKernel(xStar, x, hyperParam)
K.x.star <- SquaredExpKernel(x, xStar, hyperParam)
K.2star <- SquaredExpKernel(xStar, xStar, hyperParam)
K.inv <- solve(K.prior + sigmaNoise * diag(nrow(K.prior)))
f.mean <- K.star.x %*% K.inv %*% y
f.cov <- K.2star - K.star.x %*% K.inv %*% K.x.star
return( list(f.mean = f.mean, f.var = diag(f.cov)) )
}
choose(7,4)
choose(7,1)
choose(7,1) + choose(7,2) + choose(7,3) + choose(7,4)
choose(7,3)
choose(7,3)*4
7*7*7
installed.packages('knitr')
installed.packages("knitr")
install.packages("knitr")
library(knitr)
* Probabilistic diffusion tractography with multiple fibre orientations: What can we gain? T.E.J. Behrens,a,b,! H. Johansen Berg,a S. Jbabdi,a M.F.S. Rushworth,a,b and M.W. Woolricha
exp(0)
?Multinomial
sqrt(100000)
sqrt(1000)
sqrt(10000)
100^0.4
100^0.5
install.packages("mallet")
library("mallet")
install.packages("mallet")
library("mallet")
install.packages("tm")     # Text mining package
library("tm")
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
install.packages("XML")
library(XLM)
library(XML)
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
reuters_text_vector <- unlist(lapply(reuters, as.character))
reuters
reuters_text_vector
stopwords_en <- system.file("stopwords/english.dat", package = "tm")
stopwords_en
mallet.instances <- mallet.import(id.array = as.character(1:length(reuters_text_vector)),
text.array = reuters_text_vector,
stoplist.file = stopwords_en,
token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances
topic.model <- MalletLDA(num.topics=5, alpha.sum = 1, beta = 0.1)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
head(vocabulary)
word.freqs <- mallet.word.freqs(topic.model)
head(word.freqs)
topic.model$setAlphaOptimization(20, 50)
topic.model
topic.model$train(200)
mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 5)
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)
# What are the top words in topic 2?
# Notice that R indexes from 1 and Java from 0, so this will be the topic that mallet called topic 1.
mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 5)
mallet.top.words(topic.model, word.weights = topic.words[1,], num.top.words = 5)
mallet.top.words(topic.model, word.weights = topic.words[1,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[3,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[4,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[1,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[10,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 5)
inspect(reuters[doc.topics[,1] > 0.05][1])
#How do topics differ across different sub-corpora?
usa_articles <- unlist(meta(reuters, "places")) == "usa"
usa.topic.words <- mallet.subset.topic.words(topic.model,
subset.docs = usa_articles,
smoothed=TRUE,
normalized=TRUE)
other.topic.words <- mallet.subset.topic.words(topic.model,
subset.docs = !usa_articles,
smoothed=TRUE,
normalized=TRUE)
usa_articles <- unlist(meta(reuters, "places")) == "usa"
usa.topic.words <- mallet.subset.topic.words(topic.model,
subset.docs = usa_articles,
smoothed=TRUE,
normalized=TRUE)
other.topic.words <- mallet.subset.topic.words(topic.model,
subset.docs = !usa_articles,
smoothed=TRUE,
normalized=TRUE)
head(mallet.top.words(topic.model, usa.topic.words[1,]))
head(mallet.top.words(topic.model, other.topic.words[1,]))
#install.packages("tm")
library("tm")
# READING DATA AND PRE-PROCESS AWAY "WEIRD" ARTICLES - DON'T WORRY IF YOU CAN'T FOLLOW THIS PART
# Reading text from articles in Journal of Statistical Software using a special reader function.
# install.packages("corpus.JSS.papers", repos = "http://datacube.wu.ac.at/", type = "source")
data("JSS_papers", package = "corpus.JSS.papers")
install.packages("corpus.JSS.papers", repos = "http://datacube.wu.ac.at/", type = "source")
library(tm)
library(corpus.JSS.papers)
# READING DATA AND PRE-PROCESS AWAY "WEIRD" ARTICLES - DON'T WORRY IF YOU CAN'T FOLLOW THIS PART
# Reading text from articles in Journal of Statistical Software using a special reader function.
data("JSS_papers", package = "corpus.JSS.papers")
# Extract only papers up to 2010-08-05 and remove papers with weird (non-ASCII characters) in abstract
# Note the Encoding function returns "unknown" for ASCII text.
JSS_papers <- JSS_papers[JSS_papers[,"date"] < "2010-08-05",]
JSS_papers <- JSS_papers[sapply(JSS_papers[, "description"], Encoding) == "unknown",]
# Removing HTML markup for subscripting and greek letters etc for reading to corpus
#install.packages("XML")
library("XML")
remove_HTML_markup <- function(s) tryCatch({ doc <- htmlTreeParse(paste("<!DOCTYPE html>", s), asText = TRUE, trim = FALSE)
xmlValue(xmlRoot(doc))}, error = function(s) s)
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"], remove_HTML_markup)))
# Construct DocumentTerm matrix
# Using some linguistic pre-processing (remove stopwords and punctuation etc)
Sys.setlocale("LC_COLLATE", "C") # Language setting for the linguistic analysis
JSS_dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE,
minWordLength = 3, removeNumbers = TRUE, removePunctuation = TRUE))
JSS_dtm <- DocumentTermMatrix(corpus)
dim(JSS_dtm)
inspect(JSS_dtm[1:20,'algorithm'])
# Reducing the number of features by keeping only words with tf-idf > 0.1
#install.packages("slam")
library("slam")
term_tfidf <- tapply(JSS_dtm$v/row_sums(JSS_dtm)[JSS_dtm$i], JSS_dtm$j, mean) * log2(nDocs(JSS_dtm)/col_sums(JSS_dtm > 0))
JSS_dtm <- JSS_dtm[,term_tfidf >= 0.1] # Removing words with low tf-idf
JSS_dtm <- JSS_dtm[row_sums(JSS_dtm) > 0,] # Removing documents where no feature is present.
# Fitting a topic model with 30 topics
install.packages("topicmodels")
library(topicmodels)
LDAfit <- LDA(JSS_dtm[1:340,], k = 10, control = list(seed = 2010, iter=2000, keep=1, alpha=0.01, delta=0.01), method = "Gibbs")
LDSfit
LDAfit
plot(LDAfit@logLiks, type="l")
# Look at the results
mostLikelyTopics <- topics(LDAfit, 2)
mostLikelyWords <- terms(LDAfit, 5)
# Finding the most probable topic in each issue in volume 24
topics_v24 <- topics(LDAfit)[grep("/v24/", JSS_papers[, "identifier"])]
most_frequent_v24 <- which.max(tabulate(topics_v24))
terms(LDAfit, 10)[, most_frequent_v24]
# Predicting the last four document in the corpus (which was left out in the estimation/training)
postNewData <- posterior(LDAfit, newdata = JSS_dtm[341:344,])
perplexity(object = LDAfit, JSS_dtm[341:344,])
LDAfit20 <- LDA(JSS_dtm[1:340,], k = 20, control = list(seed = 2010, iter=2000, keep=1, alpha=0.01, delta=0.01), method = "Gibbs")
perplexity(object = LDAfit20, JSS_dtm[341:344,])
# Though it is just three hold out documents... and I'm not sure how this is calculated...
postNewData$topics
terms(LDAfit, 10)[,c(1,8)] # Document 347 talks mainly about topic 1 and 8
inspect(corpus[347])
postNewData$topics
# User input
nSim <- 10
sigmaF <- 0.1
l <- 2
#install.packages("mvtnorm")
library("mvtnorm")
# Setting up the kernel
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
n1 <- length(x1)
n2 <- length(x2)
K <- matrix(NA,n1,n2)
for (i in 1:n2){
K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
}
return(K)
}
MeanFunc <- function(x){
m <- sin(x)
return(m)
}
SimGP <- function(m = 0,K,x,nSim,...){
# Simulates nSim realizations (function) form a Gaussian process with mean m(x) and covariance K(x,x')
# over a grid of inputs (x)
n <- length(x)
if (is.numeric(m)) meanVector <- rep(0,n) else meanVector <- m(x)
covMat <- K(x,x,...)
f <- rmvnorm(n, mean = meanVector, sigma = covMat)
return(f)
}
xGrid <- seq(-5,5,length=20)
fSim <- SimGP(m=MeanFunc, K=SquaredExpKernel, x=xGrid, nSim, sigmaF, l)
plot(xGrid, fSim[1,], type="l", ylim = c(-3,3))
for (i in 2:nSim) {
lines(xGrid, fSim[i,], type="l")
}
lines(xGrid,MeanFunc(xGrid), col = "red", lwd = 3)
# Plotting using manipulate package
#install.packages('manipulate')
library(manipulate)
plotGPPrior <- function(sigmaF, l, nSim){
fSim <- SimGP(m=MeanFunc, K=SquaredExpKernel, x=xGrid, nSim, sigmaF, l)
plot(xGrid, fSim[1,], type="l", ylim = c(-3,3), ylab="f(x)", xlab="x")
for (i in 2:nSim) {
lines(xGrid, fSim[i,], type="l")
}
lines(xGrid,MeanFunc(xGrid), col = "red", lwd = 3)
title(paste('length scale =',l,', sigmaf =',sigmaF))
}
manipulate(
plotGPPrior(sigmaF, l, nSim = 10),
sigmaF = slider(0, 2, step=0.1, initial = 1, label = "SigmaF"),
l = slider(0, 2, step=0.1, initial = 1, label = "Length scale, l")
)
library(manipulate) # Manipulate requires RStudio.
####################################################################
## Bernoulli pmf
####################################################################
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(1-p,p)
barplot(pmf, names.arg=c("0", "1"), main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameter p in Bernoulli(p) distribution")
)
nTrain <- 1000 # Number of training instances. maximum is 60000
# END USER INPUT
setwd('~/Dropbox/Teaching/ProbStatUProg/')  # Set working directory
load('Data/mnist/mnist')                    # Load mnist data (http://yann.lecun.com/exdb/mnist/).
# Contains the two lists train and test.
# train$x[3,] contains the 28*28=784 grayscale (0-255)
# pixels for the third data point in the training data
# train$y[3] tell us that this is four (4).
train$x <- train$x/255      # Just standardizing to [0,1] for numerical stability
test$x <- test$x/255        # Just standardizing to [0,1] for numerical stability
# Plotting the first 6 training data examples, just to see what we are dealing with.
par(mfrow=c(2,3))
image(matrix(train$x[1,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(train$x[2,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(train$x[3,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(train$x[4,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(train$x[5,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(train$x[6,], nrow = 28)[,28:1], col=gray(12:1/12))
28*28
library(glmnet)              # loading the glmnet package/module in memory
fitMultiReg = glmnet(x = train$x[1:nTrain,], y = train$y[1:nTrain], family = "multinomial",
type.multinomial = "grouped", standardize = FALSE, lambda = exp(-6))
# Plotting the first 6 test examples
par(mfrow=c(2,3))
image(matrix(test$x[1,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(test$x[2,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(test$x[3,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(test$x[4,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(test$x[5,], nrow = 28)[,28:1], col=gray(12:1/12))
image(matrix(test$x[6,], nrow = 28)[,28:1], col=gray(12:1/12))
predict(fitMultiReg, newx = test$x[1:6,], s = exp(-6), type = "class")
test$y[1:6] # The truth
# But we can also compute the probabilities over all digits:
predClassProbs <- predict(fitMultiReg, newx = test$x[1:6,], s = exp(-6), type = "response")
predClassProbs[1:6,,1]
# Evaluating multinomial regression predictions on all test data
round(predClassProbs[1:6,,1],2)
