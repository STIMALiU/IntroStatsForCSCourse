}
trueDensity <- outer(evalPoints[,1],evalPoints[,2],bivariate)
contour(evalPoints[,1], evalPoints[,2], trueDensity, theta = 55, phi = 30, r = 40, d = 0.1, expand = 0.5,
ltheta = 90, lphi = 180, shade = 0.4, ticktype = "detailed", nticks=5)
trueDensity <- outer(evalPoints[,1],evalPoints[,2],bivariate)
trueDensity
trueDensity <- outer(evalPoints[,1],evalPoints[,2],bivariate)
# Trick to compute the true density
bivariate <- function(x,y){
return (dmvnorm(c(x,y), mean = mu, sigma = Sigma))
}
trueDensity <- outer(evalPoints[,1],evalPoints[,2],bivariate)
contour(evalPoints[,1], evalPoints[,2], trueDensity, theta = 55, phi = 30, r = 40, d = 0.1, expand = 0.5,
ltheta = 90, lphi = 180, shade = 0.4, ticktype = "detailed", nticks=5)
source('~/Dropbox/Teaching/BayesLearning/Code/GibbsBivariateNormal.R')
# Setup
mu1 <- 1
mu2 <- -1
rho <- 0.9
mu <- c(mu1,mu2)
Sigma = matrix(c(1,rho,rho,1),2,2)
nDraws <- 1000 # Number of draws
library(MASS) # To access the mvrnorm() function
# Direct sampling from bivariate normal distribution
directDraws <- mvrnorm(nDraws, mu, Sigma)
# Gibbs sampling
gibbsDraws <- matrix(0,nDraws,2)
theta2 <- 0
for (i in 1:nDraws){
# Update theta1 given theta2
theta1 <- rnorm(1, mean = mu1 + rho*(theta2-mu2), sd = sqrt(1-rho^2))
gibbsDraws[i,1] <- theta1
# Update theta2 given theta1
theta2 <- rnorm(1, mean = mu2 + rho*(theta1-mu1), sd = sqrt(1-rho^2))
gibbsDraws[i,2] <- theta2
}
gibbsDraws
head(gibbsDraws)
hist(gibbsDraws[,1])
hist(gibbsDraws[,1])
hist(gibbsDraws[100:1000,1])
mean(gibbsDraws)
mean(gibbsDraws[,1])
mean(gibbsDraws[,2])
mean(sin(gibbsDraws[,2])
)
source('~/Dropbox/Teaching/BayesLearning/Code/GibbsBivariateNormal.R')
source('~/Dropbox/Teaching/BayesLearning/Code/GibbsBivariateNormal.R')
P = matrix(c(0.8,0.1,0.1,0.2,0.6,0.2,0.3,0.3,0.4),3,3, byrow=TRUE)
P%*%P
P%*%P%*%P%*%P%*%P%*%P%*%P%*%P
P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P
P^10
P%^%100
library(expm)
P%^%100
P%^%2
rm(list=ls())
library(rstan)
nBurnin <- 1000
nIter <- 2000
rstanSeedModel<-'
data {
int<lower=0> N; ## Number of observations
int<lower=0> r[N]; ## Number of successes
int<lower=0> n[N]; ## Numer of trials
vector[N] x1; ## Covariate 1
vector[N] x2; ## Covariate 2
}
parameters {
real alpha0;
real alpha1;
real alpha2;
real<lower=0> tau;
vector[N] b;
}
transformed parameters {
real<lower=0> sigma;
sigma <- 1.0 / sqrt(tau);
}
model {
## Priors
alpha0 ~ normal(0.0,1.0E3);
alpha1 ~ normal(0.0,1.0E3);
alpha2 ~ normal(0.0,1.0E3);
tau ~ gamma(1.0E-3,1.0E-3);
## Model
b ~ normal(0.0, sigma);
r ~ binomial_logit(n, alpha0 + alpha1 * x1 + alpha2 * x2 + b);
}
'
# Data
seedsData <- list(N = 21,
r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10, 8, 10, 8, 23, 0, 3, 22, 15, 32, 3),
n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7),
x1 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
x2 = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1))
fit1<-stan(model_code=rstanSeedModel,
data=seedsData,
warmup=nBurnin,
iter=(nBurnin+nIter),
chains=4)
fit1
print(fit1,digits_summary=3)
plot(fit1)
# Extract parameters samples
fit1ParSamples<-extract(fit1,permuted=FALSE)
dim(fit1ParSamples)
fit2 <- stan(fit=fit1,data=seedsData,warmup=nBurnin,iter=(nIter+nBurnin)*10,thin=10)
fit2ParSamples<-extract(fit2,permuted=FALSE)
# Reuse the model
print(fit2,digits_summary=3)
dim(fit2ParSamples)
print(fit1,digits_summary=3)
plot(fit1)
x = c(1,1,0,0,1,1,1,1,0,1)
n = length(x)
a = 1
b = 1
BernBetaData <- list(n = length(x), x=x, a=1, b=1)
# Model
BernBetaStanModel <- '
data {
int<lower=0> n;
int<lower=0,upper=1> x[n];
real<lower=0> a;
real<lower=0> b;
}
parameters {
real<lower=0,upper=1> p;
}
model {
p ~ beta(a,b);
for (i in 1:n)
x[i] ~ bernoulli(p);
}
'
# Do the fitting of the model
fit1<-stan(model_code=BernBetaStanModel,
data=BernBetaData,
warmup=1000,
iter=2000,
chains=4)
print(fit1,digits_summary=3)
plot(fit1)
res <- extract(fit1)
pSeq <- seq(0,1,by=0.01)
par(mfrow = c(1,1))
hist(res$p, 40, freq = FALSE, main = 'Posterior of p - all chains') # histogram of draws of a from the first Markov Chain
lines(pSeq, dbeta(pSeq, shape1 = sum(x) + a, shape2 = n - sum(x) + b), col = "red")
legend("topleft", inset=.05, legend = c('MCMC approximation','True density'), lty =c(1,1),col=c('black','red'))
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
geyser
data(geyser)
rawData <- read.table('/home/mv/Dropbox/Teaching/BayesLearn2012/Code/CanadianWages.dat', header = TRUE)
library(stats)
faithful
geyser
head(faithful)
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
x <- faithful$eruptions
#x <- as.matrix(rawData['duration'])
# Model options
nComp <- 3    # Number of mixture components
# Prior options
alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(0,nComp) # Prior mean of theta
tau2Prior <- rep(10,nComp) # Prior std theta
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
data(geyser)
data(faithful)
geyser
alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(0,nComp) # Prior mean of theta
tau2Prior <- rep(10,nComp) # Prior std theta
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
nu0 <- rep(10,nComp) # degrees of freedom for prior on sigma2
# MCMC options
nIter <- 1000 # Number of Gibbs sampling draws
# Plotting options
plotFit <- TRUE
lineColors <- c("blue", "green", "magenta", 'yellow')
sleepTime <- 0.1 # Adding sleep time between iterations for plotting
################   END USER INPUT ###############
###### Defining a function that simulates from the
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
nCat <- length(param)
thetaDraws <- matrix(NA,nCat,1)
for (j in 1:nCat){
thetaDraws[j] <- rgamma(1,param[j],1)
}
thetaDraws = thetaDraws/sum(thetaDraws) # Diving every column of ThetaDraws by the sum of the elements in that column.
return(thetaDraws)
}
# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
n <- dim(S)[1]
alloc <- rep(0,n)
for (i in 1:n){
alloc[i] <- which(S[i,] == 1)
}
return(alloc)
}
# Initial value for the MCMC
nObs <- length(x)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
theta <- quantile(x, probs = seq(0,1,length = nComp))
sigma2 <- rep(var(x),nComp)
probObsInComp <- rep(NA, nComp)
# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
P = matrix(c(0.8,0.1,0.1,0.2,0.6,0.2,0.3,0.3,0.4),3,3, byrow=TRUE) # Define a transition matrix
#####   END USER INPUT    #####
install.packages("expm") # To get the matrix power
library(expm)
P
P%*%P
P%*%P%*%P
P%*%P%*%P%*%P
P%*%P%*%P%*%P%*%P
P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P
P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P%*%P
x <- faithful$eruptions
hist(x)
hist(x,30)
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
dim(c)
dim(x)
x
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
gammaln
lngamma
?gamma
?lgamma
LogMargLikeGeo <- function(y,alpha1,beta1){
yBar <- mean(y)
n <- length(y)
logMargLike <- lgamma(alpha1+beta1) -lgamma(alpha1) -lgamma(beta1) + lgamma(n + alpha1) + lgamma(n*yBar + beta1) - lgamma(n + n*yBar + alpha1 + beta1)
}
LogMargLikeGeo(c(0,0),alpha1 = 2, beta1 =4)
lml <- LogMargLikeGeo(c(0,0),alpha1 = 2, beta1 =4)
lml
LogMargLikePois <- function(y,alpha2,beta2){
yBar <- mean(y)
n <- length(y)
logMargLike <- lgamma(n*yBar + alpha2) + alpha2*log(beta2) - lgamma(alpha2) - (n*yBar + alpha2)*log(n+beta2) - sum(lgamma(y+1))
}
lml <- LogMargLikePois(c(0,0),alpha1 = 2, beta1 =4)
lml <- LogMargLikePois(c(0,0),alpha2 = 2, beta2 =4)
lml
rpois
source('~/Dropbox/Teaching/BayesLearning/Code/MainGeoVsPois.R')
source('~/Dropbox/Teaching/BayesLearning/Code/MainGeoVsPois.R')
PostProbs
source('~/Dropbox/Teaching/BayesLearning/Code/MainGeoVsPois.R')
source('~/Dropbox/Teaching/BayesLearning/Code/MainGeoVsPois.R')
source('~/Dropbox/Teaching/BayesLearning/Code/MainGeoVsPois.R')
source('~/Dropbox/Teaching/BayesLearning/Code/MainGeoVsPois.R')
library("mvtnorm")
nSim <- 10
sigmaF <- 0.1
l <- 2
#install.packages("mvtnorm")
library("mvtnorm")
# Setting up the kernel
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
n <- length(x1)
K <- matrix(NA,n,n)
for (i in 1:n){
K[,i] <- sigmaF*exp(-0.5*( (x1-x2[i])/l)^2 )
}
return(K)
}
MeanFunc <- function(x){
m <- sin(x)
return(m)
}
SimGP <- function(m = 0,K,x,nSim,...){
# Simulates nSim realizations (function) form a Gaussian process with mean m(x) and covariance K(x,x')
# over a grid of inputs (x)
n <- length(x)
if (is.numeric(m)) meanVector <- rep(0,n) else meanVector <- m(x)
covMat <- K(x,x,...)
f <- rmvnorm(n, mean = meanVector, sigma = covMat)
return(f)
}
xGrid <- seq(-5,5,length=20)
fSim <- SimGP(m=MeanFunc, K=SquaredExpKernel, x=xGrid, nSim, sigmaF, l)
plot(xGrid, fSim[1,], type="l", ylim = c(-3,3))
for (i in 2:nSim) {
lines(xGrid, fSim[i,], type="l")
}
lines(xGrid,MeanFunc(xGrid), col = "red", lwd = 3)
# Plotting using manipulate package
library(manipulate)
plotGPPrior <- function(sigmaF, l, nSim){
fSim <- SimGP(m=MeanFunc, K=SquaredExpKernel, x=xGrid, nSim, sigmaF, l)
plot(xGrid, fSim[1,], type="l", ylim = c(-3,3))
for (i in 2:nSim) {
lines(xGrid, fSim[i,], type="l")
}
lines(xGrid,MeanFunc(xGrid), col = "red", lwd = 3)
title(paste('l =',l,', sigmaf =',sigmaF))
}
manipulate(
plotGPPrior(sigmaF, l, nSim = 10),
sigmaF = slider(0, 2, step=0.1, initial = 1, label = "SigmaF"),
l = slider(0, 2, step=0.1, initial = 1, label = "Length scale, l")
)
exp(3.4)
1/exp(3.4)
exp(-3.4)
rm(list=ls())
library(rstan)
nBurnin <- 1000
nIter <- 2000
rstanSeedModel<-'
data {
int<lower=0> N; ## Number of observations
int<lower=0> r[N]; ## Number of successes
int<lower=0> n[N]; ## Numer of trials
vector[N] x1; ## Covariate 1
vector[N] x2; ## Covariate 2
}
parameters {
real alpha0;
real alpha2;
real alpha1;
real<lower=0> tau;
vector[N] b;
}
transformed parameters {
real<lower=0> sigma;
sigma <- 1.0 / sqrt(tau);
}
model {
## Priors
alpha0 ~ normal(0.0,1.0E3);
alpha1 ~ normal(0.0,1.0E3);
alpha2 ~ normal(0.0,1.0E3);
tau ~ gamma(1.0E-3,1.0E-3);
## Model
b ~ normal(0.0, sigma);
r ~ binomial_logit(n, alpha0 + alpha1 * x1 + alpha2 * x2 + b);
}
'
# Data
seedsData <- list(N = 21,
r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10, 8, 10, 8, 23, 0, 3, 22, 15, 32, 3),
n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7),
x1 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
x2 = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1))
fit1<-stan(model_code=rstanSeedModel,
data=seedsData,
warmup=nBurnin,
iter=(nBurnin+nIter),
chains=4)
fit1
print(fit1,digits_summary=3)
plot(fit1)
fit1ParSamples<-extract(fit1,permuted=FALSE)
dim(fit1ParSamples)
# Reuse the model
fit2 <- stan(fit=fit1,data=seedsData,warmup=nBurnin,iter=(nIter+nBurnin)*10,thin=10)
print(fit2,digits_summary=3)
fit2ParSamples<-extract(fit2,permuted=FALSE)
dim(fit2ParSamples)
source('~/Dropbox/Teaching/BayesLearning/Code/GaussianProcesses.R')
prec = 0.25;recall = 0.56;2*(prec*recall)/(prec+recall)
prec = 0.38;recall = 0.89;2*(prec*recall)/(prec+recall)
prec = 0.65;recall = 0.23;2*(prec*recall)/(prec+recall)
prec = 0.67;recall = 0.06;2*(prec*recall)/(prec+recall)
setwd('Dropbox/Teaching/ProbStatUProg/Code/')
library(manipulate)
####################################################################
## Bernoulli pmf
####################################################################
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
bar(xGrid, pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(xGrid, pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
barplot(c(40,20))
barplot(c(0.4,0.6))
BernPlot(0.4)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf)
#barplot(pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
#ylab = 'pmf', main = 'Bernoulli(p) density')
}
BernPlot(0.4)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf)
#barplot(pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
#ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf, names.arg=c("0", "1"))
#barplot(pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
#ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf, names.arg=c("0", "1"), ylim <- c(0, 1))
#barplot(pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
#ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf, names.arg=c("0", "1"), xlim <- c(0, 1))
#barplot(pmf, col = "blue", ylim <- c(0, 1), xlab = "x",
#ylab = 'pmf', main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
BernPlot <- function(p){
xGrid <- c(0, 1)
pmf = c(p,1-p)
barplot(pmf, names.arg=c("0", "1"), main = 'Bernoulli(p) density')
}
manipulate(
BernPlot(p),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameterp in Bernoulli(p) distribution")
)
choose(3,2)
choose(4,2)
n <- 4
xGrid <- seq(0, n)
xGrid
choose(n,xGrid)
choose(n,xGrid)*p^xGrid*(1-p)^(n-xGrid)
p = .3
choose(n,xGrid)*p^xGrid*(1-p)^(n-xGrid)
sum(choose(n,xGrid)*p^xGrid*(1-p)^(n-xGrid))
BinomialPlot <- function(n,p){
xGrid <- seq(0, n)
pmf = choose(n,xGrid)*p^xGrid*(1-p)^(n-xGrid)
barplot(pmf, names.arg=xGrid, main = 'Binomial(n,p) distribution')
}
manipulate(
BinomialPlot(n,p),
n = slider(0, 1000, step=1, initial = 10, label = "The hyperparameter n in Binomial(n,p)  distribution"),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameter p in Binomial(n,p)  distribution")
)
BinomialPlot <- function(n,p){
xGrid <- seq(0, n)
pmf = choose(n,xGrid)*p^xGrid*(1-p)^(n-xGrid)
barplot(pmf, names.arg=xGrid, main = 'Binomial(n,p) distribution')
}
manipulate(
BinomialPlot(n,p),
n = slider(0, 100, step=1, initial = 10, label = "The hyperparameter n in Binomial(n,p)  distribution"),
p = slider(0, 1, step=0.05, initial = 0.5, label = "The hyperparameter p in Binomial(n,p)  distribution")
)
?rbinom
?dbinom
